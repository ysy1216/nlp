{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0695,  0.2277, -0.9907, -0.5617,  0.4837],\n",
      "         [ 0.3208, -0.3333,  0.4655,  0.2692, -0.5449],\n",
      "         [-0.9745,  0.6336, -0.7371,  0.6157, -1.5140]]])\n",
      "tensor([[[ 0.6994,  0.9750,  2.3658,  0.0056, -1.4974,  1.3758],\n",
      "         [ 0.1764, -0.1875, -0.0397,  1.5075,  0.5834, -1.7453],\n",
      "         [-0.0845,  0.2301, -1.2351, -0.3110,  0.6215,  0.2494]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.7691, -0.4026, -0.5119,  0.7590,  0.7891,  0.8076],\n",
       "          [-0.0718, -0.5620,  0.3941,  0.7027, -0.1661,  0.4710],\n",
       "          [ 0.0847,  0.0332,  0.3617, -0.2823,  0.1825,  0.4442]]],\n",
       "        grad_fn=<StackBackward>),\n",
       " tensor([[[-0.7691, -0.4026, -0.5119,  0.7590,  0.7891,  0.8076],\n",
       "          [-0.0718, -0.5620,  0.3941,  0.7027, -0.1661,  0.4710],\n",
       "          [ 0.0847,  0.0332,  0.3617, -0.2823,  0.1825,  0.4442]]],\n",
       "        grad_fn=<StackBackward>),\n",
       " torch.Size([1, 3, 6]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "rnn=nn.RNN(5,6,1) \n",
    "# input_size: 输入张量x中特征维度的大小. 5\n",
    "# hidden_size: 隐层张量h中特征维度的大小.6\n",
    "# num_layers: 隐含层的数量 1\n",
    "input=torch.randn(1,3,5)#sequence_length  batch_size input_size\n",
    "print(input)\n",
    "h0=torch.randn(1,3,6) #num_layers*num_directions(层数*网络方向数) batch_size hidden_size \n",
    "print(h0)\n",
    "output,hn=rnn(input,h0)\n",
    "output,hn,hn.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0943, -0.0864, -0.1128,  0.1550, -0.1884, -0.3071],\n",
       "         [-0.3581, -0.2900, -0.3129,  0.0009,  0.1982, -0.1814],\n",
       "         [ 0.6345, -0.3270,  0.3803, -0.5738,  0.2252, -0.1469]]],\n",
       "       grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义LSTM的参数含义: (input_size, hidden_size, num_layers)\n",
    "# 定义输入张量的参数含义: (sequence_length, batch_size, input_size)\n",
    "# 定义隐藏层初始张量和细胞初始状态张量的参数含义:\n",
    "# (num_layers * num_directions, batch_size, hidden_size)\n",
    "# input: 输入张量x. #sequence_length  batch_size input_size\n",
    "# h0: 初始化的隐层张量h. #num_layers*num_directions(层数*网络方向数) batch_size hidden_size\n",
    "# c0: 初始化的细胞状态张量c.\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "lstm=nn.LSTM(5,6,2)\n",
    "input= torch.randn(1,3,5)\n",
    "h0=torch.randn(2,3,6)\n",
    "c0=torch.randn(2,3,6)\n",
    "output,(hn,cn)=lstm(input,(h0,c0))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1380, -0.2547,  0.4015, -0.3906,  0.0390,  0.6201],\n",
       "         [ 0.3024,  0.3568,  0.0212, -0.3250, -0.1537,  0.8749],\n",
       "         [ 0.7009,  0.2588,  0.7416, -0.9569,  0.6672, -0.4508]]],\n",
       "       grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.GRU类初始化主要参数解释:\n",
    "# input_size: 输入张量x中特征维度的大小.\n",
    "# hidden_size: 隐层张量h中特征维度的大小.\n",
    "# num_layers: 隐含层的数量.\n",
    "# bidirectional: 是否选择使用双向LSTM, 如果为True, 则使用; 默认不使用.\n",
    "# nn.GRU类实例化对象主要参数解释:\n",
    "# input: 输入张量x.#sequence_length  batch_size input_size\n",
    "# h0: 初始化的隐层张量h. #num_layers*num_directions(层数*网络方向数) batch_size hidden_size\n",
    "GRU=nn.GRU(5,6,2)\n",
    "input=torch.randn(1,3,5)\n",
    "h0=torch.randn(2,3,6)\n",
    "output,hn=GRU(input,h0)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bmm运算\n",
    "input= torch.randn(10,3,4)\n",
    "mat2=torch.randn(10,4,5)\n",
    "res=torch.bmm(input,mat2)\n",
    "res.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_categories: 18\n",
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "# 第一步: 导入必备的工具包\n",
    "from io import open #导入打开方法\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "import unicodedata\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 第二步: 对data文件中的数据进行处理，满足训练要求.\n",
    "\n",
    "all_letters=string.ascii_letters+\" .,;'\"# 获取所有常用字符包括字母和常用标点\n",
    "n_letters=len(all_letters)# 获取常用字符数量\n",
    "# print(\"n_letter:\",n_letters)\n",
    "def unicodeToAscii(s):\n",
    "    # 字符规范化之unicode转Ascii函数:# 我们认为这个函数的作用就是去掉一些语言中的重音标记\n",
    "    # 如: Ślusàrski ---> Slusarski\n",
    "    return  ''.join(\n",
    "        c for  c in unicodedata.normalize('NFD',s)\n",
    "        if unicodedata.category(c)!='Mn' and c in all_letters\n",
    "    )\n",
    "# s = \"Ślusàrski\"\n",
    "# a = unicodeToAscii(s)\n",
    "# print(a)\n",
    "data_path='./data/data/names/'\n",
    "# 构建一个从持久化文件中读取内容到内存的函数/:  #磁盘文件就是持久化文件\n",
    "def  readlines(filename):\n",
    "    \"\"\"从文件中读取每一行加载到内存中形成列表\"\"\"\n",
    "    # 打开指定文件并读取所有内容, 使用strip()去除两侧空白符, 然后以'\\n'进行切分\n",
    "    lines = open(filename,encoding='utf-8').read().strip().split('\\n')\n",
    "    #每一个lines列表中的名字进行ascii转换\n",
    "    return [unicodeToAscii(line) for line  in  lines]\n",
    "# filename = data_path +'Chinese.txt'\n",
    "# lines =readlines(filename)\n",
    "# print(lines[:20])\n",
    "\n",
    "\n",
    "# 构建人名类别（所属的语言）列表与人名对应关系字典:\n",
    "# 构建的category_lines形如：{\"English\":[\"Lily\", \"Susan\", \"Kobe\"], \"Chinese\":[\"Zhang San\", \"Xiao Ming\"]}\n",
    "category_lines = {}\n",
    "# all_categories形如： [\"English\",...,\"Chinese\"]\n",
    "all_categories = []\n",
    "\n",
    "# 读取指定路径下的txt文件， 使用glob，path中可以使用正则表达式\n",
    "for filename in glob.glob(data_path+'*txt'):\n",
    "    # 获取每个文件的文件名, 就是对应的名字类别\n",
    "    category=os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines=readlines(filename)\n",
    "    category_lines[category]=lines\n",
    "n_categories=len(all_categories)\n",
    "print('n_categories:',n_categories)\n",
    "# 随便查看其中的一些内容\n",
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将人名转化为对应onehot张量表示:\n",
    "# 将字符串(单词粒度)转化为张量表示，如：\"ab\" --->\n",
    "# tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "#        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0.]]])\n",
    "\n",
    "def lineToTensor(line):\n",
    "    \"\"\"将人名转化为对应onehot张量表示, 参数line是输入的人名\"\"\"\n",
    "    # 首先初始化一个0张量, 它的形状(len(line), 1, n_letters) \n",
    "    # 代表人名中的每个字母用一个1 x n_letters的张量表示.\n",
    "    tensor=torch.zeros(len(line),1,n_letters)#n_letters多少个字母\n",
    "    for li,letter in enumerate(line):\n",
    "        tensor[li][0][all_letters.find(letter)]=1\n",
    "    return tensor\n",
    "# line='Bai'\n",
    "# line_tensor=lineToTensor(line)\n",
    "# print(line_tensor)\n",
    "\n",
    "# 第三步: 构建RNN模型\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_layers=1):\n",
    "        \"\"\"初始化函数中有4个参数, 分别代表RNN输入最后一维尺寸, RNN的隐层最后一维尺寸, RNN层数\"\"\"\n",
    "        super(RNN,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layes=num_layers\n",
    "        #实例化RNN\n",
    "        self.rnn=nn.RNN(input_size,hidden_size,num_layers)\n",
    "        #实例化线性层  #将rnn的输出维度 转换成指定的outputsize的指定维度\n",
    "        self.linear=nn.Linear(hidden_size,output_size)\n",
    "        #实例化nn中的softmax  #获得类别结果\n",
    "        self.softmax=nn.LogSoftmax(dim=-1)#\n",
    "\n",
    "    def forward(self,input,hidden):\n",
    "        \"\"\"完成传统RNN中的主要逻辑, 输入参数input代表输入张量, 它的形状是1 x n_letters\n",
    "        hidden代表RNN的隐层张量, 它的形状是self.num_layers x 1 x self.hidden_size\"\"\"\n",
    "        # 因为预定义的nn.RNN要求输入维度一定是三维张量, 因此在这里使用unsqueeze(0)扩展一个维度\n",
    "        input=input.unsqueeze(0)\n",
    "        rr,hn=self.rnn(input,hidden)\n",
    "        return self.softmax(self.linear(rr)),hn\n",
    "\n",
    "    def initHidden(self):\n",
    "        \"\"\"初始化隐层张量\"\"\"\n",
    "        # 初始化一个（self.num_layers, 1, self.hidden_size）形状的0张量    \n",
    "        return torch.zeros(self.num_layes,1,self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用nn.LSTM构建完成LSTM使用类\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "\n",
    "        #实例化\n",
    "        self.lstm=nn.LSTM(input_size,hidden_size,num_layers)\n",
    "        #实例化线性层，控制输出维度\n",
    "        self.linear=nn.Linear(hidden_size,out_features=output_size)\n",
    "        #获得线性层的输出结果\n",
    "        self.softmax=nn.LogSoftmax(dim=-1)\n",
    "\n",
    "\n",
    "    def forward(self,input,hidden,c):\n",
    "        # c  cell 细胞状态\n",
    "        input=input.unsqueeze(0)\n",
    "        rr,(hn,c)=self.lstm(input,(hidden,c))\n",
    "        return self.softmax(self.linear(rr)),hn,c\n",
    "\n",
    "    def initHiddenAndC(self):\n",
    "        c=hidden=torch.zeros(self.num_layers,1,self.hidden_size)\n",
    "        return hidden,c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用nn.GRU构建完成传统RNN使用类\n",
    "\n",
    "# GRU与传统RNN的外部形式相同, 都是只传递隐层张量, 因此只需要更改预定义层的名字\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_layers=1):\n",
    "        super(GRU,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "\n",
    "        self.gru=nn.GRU(input_size,hidden_size,num_layers)\n",
    "        self.linear=nn.Linear(hidden_size,output_size)\n",
    "        self.softmax=nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def  forward(self,input,hidden):\n",
    "        input=input.unsqueeze(0)\n",
    "        rr,hn=self.gru(input,hidden)\n",
    "        return self.softmax(self.linear(rr)),hn\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers,1,self.hidden_size) #3ceng\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实例化参数\n",
    "#因为是onehot编码，输入张量就是最后一维度，n_letters\n",
    "input_size=n_letters\n",
    "n_hidden=128\n",
    "output_size=n_categories #shuchu 总类别数\n",
    "num_layers=1#默认"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假如我们以一个字母B作为RNN的首次输入, 它通过lineToTensor转为张量\n",
    "# 因为我们的lineToTensor输出是三维张量, 而RNN类需要的二维张量 但是foward内填充了一位\n",
    "# 因此需要使用squeeze(0)降低一个维度\n",
    "input = lineToTensor('B').squeeze(0)\n",
    "\n",
    "# 初始化一个三维的隐层0张量, 也是初始的细胞状态张量\n",
    "hidden = c = torch.zeros(1, 1, n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn=RNN(n_letters,n_hidden,n_categories)\n",
    "lstm=LSTM(n_letters,n_hidden,n_categories)\n",
    "gru=GRU(n_letters,n_hidden,n_categories)\n",
    "rnn_output,next_hidden=rnn(input,hidden)\n",
    "lstm_output,next_hidden,c=lstm(input,hidden,c)\n",
    "gru_output,next_hidden=gru(input,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第四步: 构建训练函数并进行训练\n",
    "# 从输出结果中获得指定类别函数\n",
    "def categoryFromOutput(output):\n",
    "    \"\"\"从输出结果中获得指定类别, 参数为输出张量output\"\"\"\n",
    "    # 从输出张量中返回最大的值和索引对象, 我们这里主要需要这个索引\n",
    "    top_n,top_i = output.topk(1)# top_i对象中取出索引的值 topk返回值和索引\n",
    "    category_i=top_i[0].item()\n",
    "    return all_categories[category_i],category_i # 根据索引值获得对应语言类别, 返回语言类别和索引值\n",
    "\n",
    "# output=gru_output\n",
    "# category,category_i=categoryFromOutput(output)\n",
    "# print(category,category_i)\n",
    "\n",
    "# 随机生成训练数据:.\n",
    "def randomTrainingExample():\n",
    "    #随机选择一个类别\n",
    "    category=random.choice(all_categories)\n",
    "    #该类别选择一个名字\n",
    "    line= random.choice(category_lines[category])\n",
    "    #将类别封装成tensor\n",
    "    category_tensor=torch.tensor([all_categories.index(category)],dtype=torch.long)\n",
    "    #将名字转换成onehot张量表示\n",
    "    line_tensor=lineToTensor(line)\n",
    "    return category,line,category_tensor,line_tensor\n",
    "\n",
    "\n",
    "# # 我们随机取出十个进行结果查看\n",
    "# for i in range(10):\n",
    "#     category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "#     print('category =', category, '/ line =', line, '/ category_tensor =', category_tensor,'/ line_tensor',line_tensor)\n",
    "\n",
    "# 构建传统RNN训练函数:\n",
    "criterion=nn.NLLLoss()\n",
    "learning_rate=0.005\n",
    "def trainRNN(category_tensor,line_tensor):\n",
    "    hidden=rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    # 下面开始进行训练, 将训练数据line_tensor的每个字符逐个传入rnn之中, 得到最终结果\n",
    "    for i in range(line_tensor):\n",
    "        output,hidden=rnn(line_tensor[i],hidden) #hidden循环 只需要最后一个outPUT\n",
    "    \n",
    "    loss=criterion(output.squeeze(0),category_tensor)\n",
    "    loss.backward()\n",
    "    for p in rnn.parameters():\n",
    "        # 将参数的张量表示与参数的梯度乘以学习率的结果相加以此来更新参数\n",
    "        p.data.add_(-learning_rate,p.grad.data)  #add_原地迭代\n",
    "    return output,loss.item()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d035531b1c35a6ae31c7922e561effbede15c1128d9f59192cba59910221207"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
